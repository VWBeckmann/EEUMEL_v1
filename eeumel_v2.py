# -*- coding: utf-8 -*-
"""EEUMEL_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_yVDloytrXVSbSX4_uVox2GZMZyqvqzr
"""

pip install requests PyPDF2 sympy fuzzywuzzy langchain-community tiktoken faiss-cpu python-Levenshtein langchain-openai geopy spacy langchain langgraph

import os
os.environ["OPENAI_API_KEY"]="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"

import os
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings

# Initialize OpenAI embeddings
embeddings = OpenAIEmbeddings()

# Path to saved FAISS index
index_path = "faiss_index"

# Check if the FAISS index already exists
if os.path.exists(index_path):
    # Load the existing FAISS index with safe deserialization enabled
    faiss_index = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
    print("FAISS index loaded from disk.")

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate

# Assuming the FAISS index and embeddings are already set up
embeddings = OpenAIEmbeddings()

# Set up the OpenAI chat model
chat_model = ChatOpenAI(temperature=0)

# Create a RetrievalQA chain
car_qa_chain = ConversationalRetrievalChain.from_llm(llm=chat_model, retriever=faiss_index.as_retriever())

# Function to get the response from the Car Manual agent using the chain
def get_car_manual_response(query):
    result = car_qa_chain.run(question=query, chat_history=chat_history)
    return result

from langchain.chains import LLMChain
import requests
from geopy.geocoders import Nominatim
import spacy

nlp = spacy.load("en_core_web_sm")

class WeatherAgent:
    def __init__(self):
        self.api_key = "70aae49e52748e4037e0549190625e07"
        self.base_url = "https://api.open-meteo.com/v1/forecast?"

        # Load spaCy model once
        self.nlp = spacy.load("en_core_web_sm")

    def get_weather(self, utterance):
        try:
            city = self.get_city_from_utterance(utterance)
            if city is None:
                return "Location not found."

            latitude, longitude = self.get_coordinates(city)
            if latitude is None or longitude is None:
                return "Location not found."

            # Correctly format API request URL
            response = requests.get(
                f"{self.base_url}latitude={latitude}&longitude={longitude}&current_weather=true"
            )

            # Handle response
            if response.status_code != 200:
                return f"Error retrieving weather data: {response.status_code}"

            data = response.json()
            temp = data["current_weather"]["temperature"]
            return f"Die aktuelle Temperatur in {city} beträgt {temp}°C."

        except Exception as e:
            return f"Fehler beim Abrufen des Wetters: {e}"

    def get_coordinates(self, city_name):
        """Fetch latitude and longitude from OpenWeatherMap's Geocoding API."""
        geocode_url = f"http://api.openweathermap.org/geo/1.0/direct?q={city_name}&limit=1&appid={self.api_key}"
        response = requests.get(geocode_url)

        if response.status_code == 200:
            data = response.json()
            if data:
                return data[0]["lat"], data[0]["lon"]

        return None, None

    def get_city_from_utterance(self, utterance):
        """Extract city name from user utterance using spaCy."""
        doc = self.nlp(utterance)
        for ent in doc.ents:
            if ent.label_ == "GPE":
                return ent.text  # Return first detected city
        return None  # No city detected

# Use LangChain's retrieval chain to handle the formatting of the weather response
weather_prompt_template = "You are an assistant that provides current weather information. Given the following weather data, respond clearly and professionally to the user's question:{context}User's question: {query}"

weather_prompt = PromptTemplate(template=weather_prompt_template, input_variables=["context", "query"])

# Use the OpenAI LLM
llm = OpenAI(model="gpt-3.5-turbo-instruct")

# Create a simple chain for weather using the retrieved context
weather_chain = LLMChain(llm=llm, prompt=weather_prompt)

# Function to get the full response from the weather agent
def get_weather_response(query, location="Berlin"):
    weather_data = WeatherAgent().get_weather(location)

    # Use the weather chain to generate a response
    response = weather_chain.run({"query": query, "context": weather_data})
    return response.strip()

class MultiAgentSystem:
    def __init__(self, weather_agent, car_manual_agent):
        self.weather_agent = weather_agent
        self.car_manual_agent = car_manual_agent

        # Define the prompt template to guide LLM in agent selection
        selection_prompt_template = "You are a highly intelligent assistant capable of answering a wide variety of questions. Given the following list of agents and their capabilities, determine which agent should handle the user query: 1. Weather Agent: Can provide current weather information for any city. 2. Car Manual Agent: Can provide details and explanations about car models and their manuals. 3. Sports Agent: Can provide live and historical sports scores, updates, and stats. User Query: {query} Choose the appropriate agent to handle this query. Respond with the agent's name ('Weather Agent', 'Car Manual Agent', 'Sports Agent'). After the agent's name put a '|' as a seperator and reformulate the request in german and in a clear way. For the weather Agent, only answer with the city."

        selection_prompt = PromptTemplate(template=selection_prompt_template, input_variables=["query"])

        # Use OpenAI LLM for selection
        chat_model = ChatOpenAI(temperature=0)

        # Create an LLMChain for agent selection
        self.selection_chain = LLMChain(llm=chat_model, prompt=selection_prompt)

    def process_query(self, query):
        selection_agent_answer = self.selection_chain.run(query=query)
        while(len(selection_agent_answer.split("|")) != 2):
            selection_agent_answer = self.selection_chain.run(query=query)
        agent_name, query_new = selection_agent_answer.split("|")

        if agent_name.strip() == "Weather Agent":
            # Fetch weather data and use the RetrievalChain to generate a response
            response = get_weather_response(query_new.strip())
        elif agent_name.strip() == "Car Manual Agent":
            # Fetch car manual info and use the RetrievalChain to generate a response
            response = get_car_manual_response(query_new)
        else:
            return "No fitting Agent found"
        return response

from flask import Flask, request, jsonify

# Initialisiere das Multi-Agent-System
multi_agent_system = MultiAgentSystem(weather_agent=None, car_manual_agent=None)

# Erstelle die Flask-App
app = Flask(__name__)

@app.route('/query', methods=['POST'])
def process_query():
    data = request.get_json()
    query = data.get("query", "")

    if not query:
        return jsonify({"error": "Query is missing"}), 400

    # Anfrage verarbeiten
    response = multi_agent_system.process_query(query)

    return jsonify({"query": query, "response": response})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)